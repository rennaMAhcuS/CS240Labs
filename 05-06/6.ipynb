{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 QUEENS PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATE = numpy array of positions of queens on the chess board. \\\n",
    "if pos is the position of the queen then row and column are pos//dim and pos%dim \\\n",
    "where dim is the dimension of the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "total = dim*dim\n",
    "\n",
    "def get_neighbours(state : np.ndarray) -> list[np.ndarray]:\n",
    "    neighbours = []\n",
    "    for pos in range(total):\n",
    "        r, c = pos//dim, pos%dim \n",
    "        conflict = False\n",
    "        for queen in state:\n",
    "            qr, qc = queen//dim, queen%dim\n",
    "            if qr==r or qc==c or abs(qr-r)==abs(qc-c):\n",
    "                conflict = True\n",
    "                break\n",
    "        \n",
    "        if not conflict:\n",
    "            new_state = np.append(state, pos)\n",
    "            new_state.sort()\n",
    "            neighbours.append(new_state)\n",
    "    \n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general A star for this problem. \\\n",
    "cost = 1 and g equal to number of edges traversed in the graph \\\n",
    "which is equal to number of queens in the present state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genA_star(initial : np.ndarray, heuristic_fn : callable) -> tuple[int, np.ndarray]:\n",
    "    numNodesExpanded = 0\n",
    "    open_list = []\n",
    "    closed_list = set()\n",
    "    heapq.heappush(open_list, (0 + heuristic_fn(initial), tuple(initial))) \n",
    "    g_scores = { tuple(initial) : 0 }\n",
    "\n",
    "    while open_list:\n",
    "        _, currStateTuple = heapq.heappop(open_list)\n",
    "        if currStateTuple in closed_list:\n",
    "            continue\n",
    "\n",
    "        closed_list.add(currStateTuple)\n",
    "        currState = np.array(currStateTuple)\n",
    "\n",
    "        if currState.size == dim:\n",
    "            return numNodesExpanded, currState.astype(int)\n",
    "\n",
    "        numNodesExpanded+=1\n",
    "        neighbours = get_neighbours(currState)\n",
    "\n",
    "        for nextState in neighbours:\n",
    "            nextStateTuple = tuple(nextState)\n",
    "            if nextStateTuple in closed_list:\n",
    "                continue\n",
    "\n",
    "            if nextStateTuple not in g_scores:\n",
    "                g_scores[nextStateTuple] = nextState.size\n",
    "                heapq.heappush(open_list, (nextState.size + heuristic_fn(nextState), nextStateTuple))\n",
    "         \n",
    "    return 0, initial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_1(state : np.ndarray):\n",
    "    return 0 \n",
    "\n",
    "def heuristic_2(state : np.ndarray):\n",
    "    return dim - state.size\n",
    "\n",
    "def heuristic_3(state : np.ndarray):\n",
    "    safe_count = 0 \n",
    "    for pos in range(total):\n",
    "        r, c = pos//dim, pos%dim\n",
    "        safe = True\n",
    "        for queen in state:\n",
    "            qr, qc = queen//dim, queen%dim\n",
    "            if qr==r or qc==c or abs(qr-r)==abs(qc-c):\n",
    "                safe = False\n",
    "                break\n",
    "        \n",
    "        if safe:\n",
    "            safe_count+=1\n",
    "    \n",
    "    return safe_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_heuristic(heuristic_name, heuristic_fn : callable):\n",
    "        initial = np.array([])\n",
    "        numNodesExpanded, goal = genA_star(initial, heuristic_fn) \n",
    "        print(f\"Number of nodes expanded using {heuristic_name} : {numNodesExpanded}\")\n",
    "        chessBoard = np.zeros((dim, dim), dtype = int)\n",
    "        for queen in goal:\n",
    "            qr, qc = queen//dim, queen%dim\n",
    "            chessBoard[qr][qc] = 1\n",
    "        for r in range(dim):\n",
    "            row = \"\"\n",
    "            for c in range(dim):\n",
    "                if chessBoard[r][c] == 0:\n",
    "                    row+=\"O \"\n",
    "                else:\n",
    "                    row+=\"X \"\n",
    "            print(row)\n",
    "        \n",
    "        print(\"-----------------------------------\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    use_heuristic(\"zero heuristic\", heuristic_1) \n",
    "    use_heuristic(\"remaining queens heuristic\", heuristic_2)\n",
    "    use_heuristic(\"number of safe places left heuristic\", heuristic_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode the target variable into the 10 classes (0-9)\n",
    "# Input shape: (N,),    Output: (N, 10)\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, np.max(Y)+1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "# Prepare data sets\n",
    "def get_inputs(dim: int = 2):\n",
    "    if dim == 1:\n",
    "        return np.array([[0], [1]])\n",
    "    array = get_inputs(dim-1)\n",
    "    zeroes = np.zeros((2**(dim-1), 1))\n",
    "    ones = np.ones((2**(dim-1), 1))\n",
    "    arr1 = np.concatenate((zeroes, array), axis=1)\n",
    "    arr2 = np.concatenate((ones, array), axis=1)\n",
    "\n",
    "    return np.concatenate((arr1, arr2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset\n",
    "train_data=pd.read_csv(r\"./mnist_train.csv\")\n",
    "test_data=pd.read_csv(r\"./mnist_test.csv\")\n",
    "\n",
    "# Preprocessing the data\n",
    "train_data=train_data.to_numpy()    # train_data shape: (60000, 785)\n",
    "test_data=test_data.to_numpy()      # test_data shape: (10000, 785)\n",
    "\n",
    "X_train=train_data[:,1:]            # X_train shape: (60000, 784)\n",
    "y_train=train_data[:,0]             # y_train shape: (60000,)\n",
    "X_test=test_data[:,1:]              # X_test shape: (10000, 784)\n",
    "y_test=test_data[:,0]               # y_test shape: (10000,)\n",
    "\n",
    "X_train = X_train / 255.0           # Normalizing the data\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "one_hot_y_train = one_hot(y_train)  # one_hot_y_train shape: (60000, 10)\n",
    "one_hot_y_test = one_hot(y_test)    # one_hot_y_test shape: (10000, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN:\n",
    "    def __init__(self, layers_size, loss=\"ce\", activation=\"relu\", learning_rate=0.01) -> None:\n",
    "        self.layers_size = layers_size\n",
    "        self.loss = loss\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.L = len(layers_size)\n",
    "\n",
    "        self.weights = [np.random.randn(layers_size[i], layers_size[i+1])*0.01 for i in range(self.L-1)]\n",
    "        self.biases = [np.zeros((1, layers_size[i+1])) for i in range(self.L-1)]\n",
    "\n",
    "    def activation_fn(self, x, deriv=False):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            sig = 1/(1+np.exp(-x))\n",
    "            return sig if not deriv else sig*(1-sig)\n",
    "        elif self.activation == \"relu\":\n",
    "            return np.maximum(x, 0) if not deriv else (x>0).astype(float)\n",
    "    \n",
    "    def loss_fn(self, y_pred, y_true, deriv=False):\n",
    "        if self.loss == \"tss\":\n",
    "            loss = 0.5*np.sum((y_pred - y_true)**2)\n",
    "            return loss if not deriv else y_pred-y_true\n",
    "        elif self.loss == \"ce\":\n",
    "            loss = -np.sum(y_true*np.log(y_pred + 1e-9))\n",
    "            return loss if not deriv else y_pred-y_true\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        ret = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return  ret/(np.sum(ret, axis=1, keepdims=True))\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = [X]\n",
    "        Z = []\n",
    "\n",
    "        for i in range(self.L-2):\n",
    "            z = np.dot(A[-1], self.weights[i]) + self.biases[i]\n",
    "            Z.append(z) \n",
    "            A.append(self.activation_fn(z))\n",
    "        \n",
    "        z = np.dot(A[-1], self.weights[-1]) + self.biases[-1]\n",
    "        Z.append(z) \n",
    "        A.append(self.softmax(z))\n",
    "\n",
    "        return A, Z\n",
    "    \n",
    "    def backward(self, X_train, y_train):\n",
    "        m = X_train.shape[0]\n",
    "        A, Z = self.forward(X_train) \n",
    "        y_pred = A[-1]\n",
    "\n",
    "        dA = self.loss_fn(y_pred, y_train, True)\n",
    "        \n",
    "        dWs = []\n",
    "        dbs = []\n",
    "        \n",
    "        for i in range(self.L-2, -1, -1):\n",
    "            dZ = dA if (self.loss == \"ce\" and i==self.L-2) else dA*self.activation_fn(Z[i], True)\n",
    "            dW = np.dot(A[i].T, dZ)/m\n",
    "            db = np.sum(dZ, axis = 0, keepdims=True)/m\n",
    "            dWs.append(dW)\n",
    "            dbs.append(db)\n",
    "            dA = np.dot(dZ, self.weights[i].T) \n",
    "        \n",
    "        dWs.reverse()\n",
    "        dbs.reverse()\n",
    "        for i in range(self.L-1):\n",
    "            self.weights[i]-=(self.learning_rate*dWs[i])\n",
    "            self.biases[i]-=(self.learning_rate*dbs[i])\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def train(self, X, y, epochs=10, batch_size=32, print_in = 2):\n",
    "        m = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(m)\n",
    "            X_perm, y_perm = X[perm], y[perm]\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X_perm[i : i+batch_size]\n",
    "                y_batch = y_perm[i : i+batch_size]\n",
    "                self.backward(X_batch, y_batch)\n",
    "            \n",
    "            if epoch%print_in == 0:\n",
    "                loss = self.loss_fn(self.forward(X)[0][-1], y)\n",
    "                losses.append(loss)\n",
    "                print(f\"Epoch : {epoch} => Loss : {loss}\")\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X)[0][-1], axis=1)\n",
    "    \n",
    "    def draw_graph(self, y_axis, x_axis):\n",
    "        plt.plot(x_axis, y_axis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN((784, 128, 64, 10), \"ce\", \"relu\", 0.01)\n",
    "print_in = 2\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = model.train(X_train, one_hot_y_train, epochs, batch_size, print_in)\n",
    "x_axis = np.arange(0, epochs, print_in)\n",
    "model.draw_graph(losses, x_axis)\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "accuracy = np.mean(y_predict == y_test)\n",
    "print(f\"Accuracy => {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR data set \n",
    "input_XOR = get_inputs(2)\n",
    "output_XOR = np.array([0, 1, 1, 0])\n",
    "output_XOR_one_hot = one_hot(output_XOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR_model = FNN((2, 4, 2), \"ce\", \"relu\", 0.01)\n",
    "epochs = 10000\n",
    "batch_size = 4\n",
    "print_in = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = XOR_model.train(input_XOR, output_XOR_one_hot, epochs, batch_size, print_in)\n",
    "x_axis = np.arange(0, epochs, print_in)\n",
    "XOR_model.draw_graph(losses, x_axis)\n",
    "\n",
    "y_predict = XOR_model.predict(input_XOR)\n",
    "accuracy = np.mean(y_predict == output_XOR)\n",
    "print(f\"Accuracy => {accuracy*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
