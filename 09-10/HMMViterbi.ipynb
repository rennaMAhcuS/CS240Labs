{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524450c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/emsr/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/emsr/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the Brown corpus and the universal tagset\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"universal_tagset\")\n",
    "# Get the tagged sentences from the Brown corpus\n",
    "tagged_sentences = list(nltk.corpus.brown.tagged_sents(tagset=\"universal\"))\n",
    "# The words converted to lower format\n",
    "tagged_sentences_lower = [[(word.lower(), tag) for word, tag in sentence] for sentence in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7447f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parts of speech\n",
    "pos = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"CONJ\": 3, \"DET\": 4, \"NOUN\": 5, \"NUM\": 6, \"PRON\": 7, \"PRT\": 8, \"VERB\": 9, \".\": 10, \"X\": 11, \"START\": 12, \"END\": 13}\n",
    "pos_keys = list(pos.keys())\n",
    "num_pos = 12\n",
    "num_tags = 14\n",
    "# The word map\n",
    "word_map = {}\n",
    "num_words = 0\n",
    "for sentence in tagged_sentences_lower:\n",
    "    for word, _ in sentence:\n",
    "        if word not in word_map:\n",
    "            word_map[word] = num_words\n",
    "            num_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a539cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_transition_matrix(dataset):\n",
    "    transition = np.ones((num_tags, num_tags))\n",
    "    \n",
    "    for sentence in dataset:\n",
    "        previous_tag = \"START\"\n",
    "        for _, tag in sentence:\n",
    "            transition[pos[previous_tag], pos[tag]] += 1\n",
    "            previous_tag = tag\n",
    "        transition[pos[previous_tag], pos[\"END\"]] += 1  # Not necessary\n",
    "    \n",
    "    return transition / transition.sum(axis=1, keepdims=True)\n",
    "\n",
    "def get_emission_matrix(dataset):\n",
    "    emission = np.ones((num_pos, num_words + 1))\n",
    "\n",
    "    for sentence in dataset:\n",
    "        for word, tag in sentence:\n",
    "            emission[pos[tag], word_map[word]] += 1\n",
    "        \n",
    "    return emission / emission.sum(axis=1, keepdims=True)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train_set, test_set):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.test_sentences = []\n",
    "        self.transition_matrix = get_transition_matrix(train_set)\n",
    "        self.emission_matrix = get_emission_matrix(train_set)\n",
    "\n",
    "    def get_test_sentences(self):\n",
    "        for sentence in self.test_set:\n",
    "            self.test_sentences.append(\" \".join(word for word, _ in sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce27f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "n = len(tagged_sentences_lower)\n",
    "sz = n // 5\n",
    "parts = []\n",
    "for i in range(5):\n",
    "    parts.append(tagged_sentences_lower[i * sz : i * sz + sz])\n",
    "\n",
    "test_set_arr = []\n",
    "\n",
    "for i in range(5):\n",
    "    train_set_arr = []\n",
    "    for j in range(5):\n",
    "        if j == i:\n",
    "            continue\n",
    "        train_set_arr.extend(parts[j])\n",
    "\n",
    "    test_set_arr = parts[i]\n",
    "\n",
    "    datasets.append(Dataset(train_set_arr, test_set_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ccbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_general(transition, emission, sentence):\n",
    "    \n",
    "    parsed_sentence = sentence.lower().split()\n",
    "    sentence_len = len(parsed_sentence)\n",
    "    \n",
    "    log_prob = np.full((sentence_len, num_pos), -np.inf)\n",
    "    prev = np.full((sentence_len, num_pos), -1)\n",
    "\n",
    "    # Determining the probabilities:\n",
    "    for state in range(num_pos):\n",
    "        log_prob[0, state] = np.log(transition[pos[\"START\"], state]) \n",
    "        if parsed_sentence[0] in word_map:\n",
    "            log_prob[0, state] += np.log(emission[state, word_map[parsed_sentence[0]]])\n",
    "        else:\n",
    "            log_prob[0, state] += np.log(emission[state, num_words])\n",
    "\n",
    "    \n",
    "    for word_idx in range(1, sentence_len):\n",
    "        for state_curr in range(num_pos):\n",
    "            for state_prev in range(num_pos):\n",
    "                new_log_prob = log_prob[word_idx - 1, state_prev] + np.log(transition[state_prev, state_curr])\n",
    "                if parsed_sentence[word_idx] in word_map:\n",
    "                    new_log_prob += np.log(emission[state_curr, word_map[parsed_sentence[word_idx]]])\n",
    "                else:\n",
    "                    new_log_prob += np.log(emission[state_curr, num_words])\n",
    "                if new_log_prob > log_prob[word_idx, state_curr]:\n",
    "                    log_prob[word_idx, state_curr] = new_log_prob\n",
    "                    prev[word_idx, state_curr] = state_prev\n",
    "\n",
    "    # Backtracking\n",
    "    rev_tags = [np.argmax(log_prob[-1])]\n",
    "    for word_idx in range(sentence_len - 1, 0, -1):\n",
    "        rev_tags.append(prev[word_idx, rev_tags[-1]])\n",
    "\n",
    "    tags = []\n",
    "    for tag_idx in rev_tags[::-1]:\n",
    "        tags.append(pos_keys[tag_idx])\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def viterbi(transition, emission, sentence):\n",
    "    sentence_len = len(sentence)\n",
    "    \n",
    "    log_prob = np.full((sentence_len, num_pos), -np.inf)\n",
    "    prev = np.full((sentence_len, num_pos), -1)\n",
    "\n",
    "    # Determining the probabilities:\n",
    "    for state in range(num_pos):\n",
    "        log_prob[0, state] = np.log(transition[pos[\"START\"], state]) \n",
    "        if sentence[0][0] in word_map:\n",
    "            log_prob[0, state] += np.log(emission[state, word_map[sentence[0][0]]])\n",
    "        else:\n",
    "            log_prob[0, state] += np.log(emission[state, num_words])\n",
    "\n",
    "    \n",
    "    for word_idx in range(1, sentence_len):\n",
    "        for state_curr in range(num_pos):\n",
    "            for state_prev in range(num_pos):\n",
    "                new_log_prob = log_prob[word_idx - 1, state_prev] + np.log(transition[state_prev, state_curr])\n",
    "                if sentence[word_idx][0] in word_map:\n",
    "                    new_log_prob += np.log(emission[state_curr, word_map[sentence[word_idx][0]]])\n",
    "                else:\n",
    "                    new_log_prob += np.log(emission[state_curr, num_words])\n",
    "                if new_log_prob > log_prob[word_idx, state_curr]:\n",
    "                    log_prob[word_idx, state_curr] = new_log_prob\n",
    "                    prev[word_idx, state_curr] = state_prev\n",
    "\n",
    "    # Backtracking\n",
    "    rev_tags = [np.argmax(log_prob[-1])]\n",
    "    for word_idx in range(sentence_len - 1, 0, -1):\n",
    "        rev_tags.append(prev[word_idx, rev_tags[-1]])\n",
    "\n",
    "    tags = []\n",
    "    for tag_idx in rev_tags[::-1]:\n",
    "        tags.append(pos_keys[tag_idx])\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6879b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting and checking the accuracy\n",
    "predicted_tags = []\n",
    "actual_tags = []\n",
    "for i in range(5):\n",
    "    for sentence in datasets[i].test_set:\n",
    "        predicted_tags.extend(viterbi(datasets[i].transition_matrix, datasets[i].emission_matrix, sentence))\n",
    "    for sentence in datasets[i].test_set:\n",
    "        actual_tags.extend(tag for _, tag in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20c3264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.98      1.00      0.99    147565\n",
      "         ADJ       0.86      0.87      0.86     83721\n",
      "         ADP       0.91      0.97      0.94    144766\n",
      "         ADV       0.88      0.87      0.87     56239\n",
      "        CONJ       0.97      0.99      0.98     38151\n",
      "         DET       0.90      0.99      0.94    137019\n",
      "        NOUN       0.94      0.89      0.91    275558\n",
      "         NUM       0.99      0.77      0.87     14874\n",
      "        PRON       0.86      0.95      0.90     49334\n",
      "         PRT       0.90      0.84      0.87     29829\n",
      "        VERB       0.96      0.91      0.94    182750\n",
      "           X       0.58      0.29      0.39      1386\n",
      "\n",
      "    accuracy                           0.93   1161192\n",
      "   macro avg       0.89      0.86      0.87   1161192\n",
      "weighted avg       0.93      0.93      0.93   1161192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(metrics.classification_report(actual_tags, predicted_tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
