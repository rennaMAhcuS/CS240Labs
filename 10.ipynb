{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/labuser/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/labuser/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import corpus\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download the Brown corpus and the universal tagset\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"universal_tagset\")\n",
    "\n",
    "# Get the tagged sentences from the Brown corpus\n",
    "tagged_sentences = list(corpus.brown.tagged_sents(tagset=\"universal\"))\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for word in sentence:\n",
    "        word[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in tagged_sentences[:10]:\n",
    "#     for word_tuple in sentence:\n",
    "#         print(word_tuple[0], end=\" \")\n",
    "#     print()\n",
    "\n",
    "pos = {\n",
    "    \"ADJ\": 1,\n",
    "    \"ADP\": 2,\n",
    "    \"ADV\": 3,\n",
    "    \"CONJ\": 4,\n",
    "    \"DET\": 5,\n",
    "    \"NOUN\": 6,\n",
    "    \"NUM\": 7,\n",
    "    \"PRON\": 8,\n",
    "    \"PRT\": 9,\n",
    "    \"VERB\": 10,\n",
    "    \".\": 11,\n",
    "    \"X\": 12,\n",
    "}\n",
    "\n",
    "rev_pos = [\"\", \"ADJ\", \"ADP\", \"ADV\", \"CONJ\", \"DET\", \"NOUN\", \"NUM\", \"PRON\", \"PRT\", \"VERB\", \".\", \"X\", \"\"]\n",
    "\n",
    "# beginning = 0, ending = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkov:\n",
    "    def __init__(self, tagged_sentences, pos, rev_pos):\n",
    "        self.tagged_sentences = tagged_sentences\n",
    "        self.pos = pos\n",
    "        self.rev_pos = rev_pos\n",
    "        self.word_map = {}\n",
    "\n",
    "        self.train_sets = []\n",
    "        self.test_sets = []\n",
    "        self.transition_matrix = []\n",
    "        self.emission_matrices = []\n",
    "        self.total_transition_matrix = []\n",
    "        self.total_emission_matrix = []\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_map(self) -> dict:\n",
    "        id = 0\n",
    "        for sentence in tagged_sentences:\n",
    "            for word, _ in sentence:\n",
    "                if word in self.word_map.keys():\n",
    "                    continue\n",
    "                self.word_map[word] = id\n",
    "                id += 1\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_transition_matrix(self, tagged_sentence) -> np.ndarray:\n",
    "        transition_matrix = np.ones((14, 14))\n",
    "        for sentence in tagged_sentence:\n",
    "            n = len(sentence)\n",
    "            for i in range(n):\n",
    "                word, tag = sentence[i]\n",
    "                tag = pos[tag]\n",
    "                if i == 0:\n",
    "                    transition_matrix[0][tag] += 1\n",
    "                elif i < n - 1:\n",
    "                    transition_matrix[tag][pos[sentence[i + 1][1]]] += 1\n",
    "                else:\n",
    "                    transition_matrix[tag][13] += 1\n",
    "\n",
    "        transition_matrix = transition_matrix / np.sum(\n",
    "            transition_matrix, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        return transition_matrix\n",
    "\n",
    "    def get_emission_matrix(self, tagged_sentence) -> np.ndarray:\n",
    "        n = len(self.word_map)\n",
    "        emission_matrix = np.ones((14, n))\n",
    "        for sentence in tagged_sentence:\n",
    "            for word, tag in sentence:\n",
    "                tag = pos[tag]\n",
    "                emission_matrix[tag][self.word_map[word]] += 1\n",
    "\n",
    "        emission_matrix = emission_matrix / np.sum(\n",
    "            emission_matrix, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        return emission_matrix\n",
    "\n",
    "    def five_fold(self):\n",
    "        n = len(self.tagged_sentences)\n",
    "        sz = n // 5\n",
    "        parts = []\n",
    "        for i in range(5):\n",
    "            parts.append(tagged_sentences[i * sz : i * sz + sz])\n",
    "        for i in range(5):\n",
    "            self.test_sets.append(parts[i])\n",
    "            train_set = []\n",
    "            for j in range(5):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                train_set.extend(parts[j])\n",
    "\n",
    "            self.train_sets.append(train_set)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_five_fold_matrices(self):\n",
    "        self.train_sets = []\n",
    "        self.test_sets = []\n",
    "        self.transition_matrix = []\n",
    "        self.emission_matrices = []\n",
    "        self.five_fold()\n",
    "        self.get_map()\n",
    "\n",
    "        self.total_emission_matrix = self.get_emission_matrix(self.tagged_sentences)\n",
    "        self.total_transition_matrix = self.get_transition_matrix(self.tagged_sentences)        \n",
    "        for i in range(5):\n",
    "            self.transition_matrix.append(\n",
    "                self.get_transition_matrix(self.train_sets[i])\n",
    "            )\n",
    "            self.emission_matrices.append(self.get_emission_matrix(self.train_sets[i]))\n",
    "\n",
    "        return\n",
    "    \n",
    "    def viterbi_sentence(self, sentence, transition_matrix : np.ndarray, emmission_matrix : np.ndarray):\n",
    "        n = len(sentence)\n",
    "        T = len(self.pos) + 2\n",
    "        V = np.full((n, T), 0) \n",
    "        B = np.full((n, T), -1)\n",
    "        const = np.log(1/len(self.word_map))\n",
    "\n",
    "        # first word\n",
    "        word = sentence[0][0]\n",
    "        word_id = self.word_map[word] if word in self.word_map else -1\n",
    "        for tag in range(1, T-1):\n",
    "            V[0, tag] = np.log(transition_matrix[0, tag]) \n",
    "            if word_id != -1:\n",
    "                V[0, tag]+=np.log(emmission_matrix[tag, word_id])\n",
    "            else:\n",
    "                V[0, tag]+=const\n",
    "\n",
    "\n",
    "        # rest all words\n",
    "        for i in range(1, n):\n",
    "            word = sentence[i][0]\n",
    "            word_id = self.word_map[word] if word in self.word_map else -1\n",
    "            for tag in range(1, T-1):\n",
    "                max_prob = -np.inf\n",
    "                back_id = -1\n",
    "                for prev in range(1, T-1):\n",
    "                    prob = V[i-1][prev] + np.log(transition_matrix[prev, tag]) \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        back_id = prev\n",
    "                \n",
    "                if word_id != -1:\n",
    "                    max_prob+=(np.log(emmission_matrix[tag, word_id]))\n",
    "                else:\n",
    "                    max_prob+=const\n",
    "                \n",
    "                if i == n-1:\n",
    "                    max_prob+=(np.log(transition_matrix[tag, T-1]))\n",
    "\n",
    "                V[i, tag] = max_prob\n",
    "                B[i, tag] = back_id\n",
    "        \n",
    "        # find last word tag and back propogate\n",
    "        last_tag = -1\n",
    "        max_prob = -np.inf\n",
    "        for tag in range(1, T-1):\n",
    "            if V[n-1, tag] > max_prob:\n",
    "                max_prob = V[n-1, tag]\n",
    "                last_tag = tag\n",
    "        \n",
    "        pos_tags = np.full((n), 0)\n",
    "        pos_tags[n-1] = last_tag\n",
    "        for i in range(n-2, -1, -1):\n",
    "            pos_tags[i] = B[i+1, pos_tags[i+1]]\n",
    "        \n",
    "        pred_tags = [self.rev_pos[tag] for tag in pos_tags]\n",
    "\n",
    "        return pred_tags\n",
    "    \n",
    "    def viterbi_algo(self, sentence):\n",
    "        return self.viterbi_sentence(sentence, self.total_emission_matrix, self.total_emission_matrix)\n",
    "\n",
    "\n",
    "    def accuracy_of_match(self, pred_labels, actual_labels):\n",
    "        n = len(pred_labels)\n",
    "        m = 0 \n",
    "        for i in range(n):\n",
    "            if pred_labels[i] == actual_labels[i]:\n",
    "                m+=1 \n",
    "        acc = (m/n)*100 \n",
    "        return acc\n",
    "    \n",
    "    def classification_report(self):\n",
    "        predicted = []\n",
    "        for i in range(5):\n",
    "            transition_matrix = self.transition_matrix[i]\n",
    "            emmission_matrix = self.emission_matrices[i]\n",
    "            for sentence in self.test_sets[i]:\n",
    "                predicted.extend(self.viterbi_sentence(sentence, transition_matrix, emmission_matrix))\n",
    "        \n",
    "        actual = []\n",
    "        for sentence in self.tagged_sentences:\n",
    "            for word in sentence:\n",
    "                actual.append(word[1])\n",
    "        \n",
    "        return actual, predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HiddenMarkov(tagged_sentences=tagged_sentences, pos=pos, rev_pos=rev_pos)\n",
    "model.get_five_fold_matrices()\n",
    "\n",
    "def actual_tags(sentence):\n",
    "    return [word[1] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'ADP', 'NUM', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.', 'ADP', 'PRON', 'ADV', 'VERB', 'DET', 'NOUN', 'ADV', 'ADP', 'NOUN', 'ADV', 'VERB', 'ADP', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "['DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.', 'ADP', 'PRON', 'ADV', 'VERB', 'ADP', 'NOUN', 'ADV', 'ADP', 'NOUN', 'ADV', 'VERB', 'ADP', 'PRON', 'VERB', 'VERB', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "94.11764705882352\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGING CODE\n",
    "transition_matrix = model.transition_matrix[0]\n",
    "emmission_matrix = model.emission_matrices[0]\n",
    "train_data = model.train_sets[0]\n",
    "test_data = model.test_sets[0]\n",
    "actual = actual_tags(train_data[6])\n",
    "pred = model.viterbi_sentence(train_data[6], transition_matrix, emmission_matrix)\n",
    "print(actual)\n",
    "print(pred)\n",
    "print(model.accuracy_of_match(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = model.classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.97      1.00      0.98    147565\n",
      "         ADJ       0.85      0.85      0.85     83721\n",
      "         ADP       0.89      0.97      0.93    144766\n",
      "         ADV       0.84      0.85      0.85     56239\n",
      "        CONJ       0.97      0.99      0.98     38151\n",
      "         DET       0.89      0.99      0.94    137019\n",
      "        NOUN       0.94      0.87      0.90    275558\n",
      "         NUM       0.99      0.70      0.82     14874\n",
      "        PRON       0.84      0.95      0.89     49334\n",
      "         PRT       0.90      0.83      0.86     29829\n",
      "        VERB       0.96      0.89      0.93    182750\n",
      "           X       0.61      0.26      0.36      1386\n",
      "\n",
      "    accuracy                           0.92   1161192\n",
      "   macro avg       0.89      0.85      0.86   1161192\n",
      "weighted avg       0.92      0.92      0.92   1161192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(actual, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  [0.98491412 0.8509025  0.93209387 0.84508466 0.97871184 0.93639309\n",
      " 0.90199898 0.81931219 0.89267798 0.86474106 0.92521884 0.36067244]\n",
      "Accuracy score :  0.9178378769402477\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(\"F1 score : \", metrics.f1_score(actual, predicted, average=None))\n",
    "print(\"Accuracy score : \", metrics.accuracy_score(actual, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
